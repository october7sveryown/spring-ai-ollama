
# Chat with local LLMs using Java, Spring AI & Ollama

This is a starter Spring Boot project for interacting with Local LLMs using Ollama and Spring AI.

#### Step 1 : Install Ollama

To get started, first you need to install [Ollama](https://ollama.ai) on your local system.

In this example, I'll be using Dolphin-Phi (as it takes up less space) however you can use all the other models like Mistral, Llama and more. You can check the list of models [here.](https://ollama.com/library)

#### Start ollama instance with dolphin-phi locally

```
  ollama run dolphin-phi
```

#### Step 2 : Run this project

Once you've cloned this project, run the project using IntelliJ or any code editor of your choice.

#### Step 3 : Test API

#### Use postman

```
  GET localhost:8080/ai/helloollama
```

#### From terminal

```
  curl http://localhost:8080/ai/helloollama
```




## Authors

- [@october7sveryown](https://www.github.com/october7sveryown)


## ðŸ”— Links
[![portfolio](https://img.shields.io/badge/my_portfolio-000?style=for-the-badge&logo=ko-fi&logoColor=white)](https://yash-thaker.framer.ai/)
[![linkedin](https://img.shields.io/badge/linkedin-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/yash-thaker)
[![twitter](https://img.shields.io/badge/twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/itsyash777)

